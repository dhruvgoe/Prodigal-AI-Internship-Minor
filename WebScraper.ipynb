{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 86\u001b[0m\n\u001b[0;32m     82\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent for article \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m \u001b[43mscrape_all_article_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m, in \u001b[0;36mscrape_all_article_content\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     40\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory Name,Article Name,Article Link,Article Content\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# CSV headers\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     43\u001b[0m     category_name \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory Name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     44\u001b[0m     category_link \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory Link\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pandas as pd \n",
    "\n",
    "# Define a function to scrape the article content from an article link\n",
    "def scrape_article_content(article_link, session):\n",
    "    try:\n",
    "        response = session.get(article_link, timeout=10)  # Set a timeout\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve {article_link}, status code: {response.status_code}\")\n",
    "            return \"N/A\"\n",
    "\n",
    "        # Parse the HTML content\n",
    "        data = bs(response.content, 'html.parser')\n",
    "\n",
    "        # Find all divs and then filter for class 'cooked'\n",
    "        divs = data.find_all('div', class_='post')\n",
    "        \n",
    "        if divs:\n",
    "            # Assuming we want the first occurrence of div with class 'cooked'\n",
    "            article_content = divs[0].get_text(strip=True)\n",
    "        else:\n",
    "            article_content = \"N/A\"\n",
    "            \n",
    "        return article_content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to retrieve content from {article_link}: {e}\")\n",
    "        return \"N/A\"\n",
    "\n",
    "# Define a function to scrape all articles from all categories and save in one file\n",
    "def scrape_all_article_content():\n",
    "    session = requests.Session()  # Create a session object for persistent connection\n",
    "    session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'})  # Add User-Agent\n",
    "    \n",
    "    output_file = 'all_articles_content.csv'  # Output file to store all data\n",
    "\n",
    "    # Open the file in write mode and add headers\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"Category Name,Article Name,Article Link,Article Content\\n\")  # CSV headers\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        category_name = row['Category Name']\n",
    "        category_link = row['Category Link']\n",
    "\n",
    "        # Ensure the URL is properly formed\n",
    "        if not category_link.startswith(\"http\"):\n",
    "            category_link = f\"https://gov.optimism.io{category_link}\"\n",
    "\n",
    "        response = None\n",
    "\n",
    "        # Retry logic with exponential backoff\n",
    "        for attempt in range(1, 4):  # Retry up to 3 times\n",
    "            try:\n",
    "                response = session.get(category_link, timeout=10)  # Set a timeout\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt}: RequestException for {category_link}: {e}. Retrying in {5 * attempt} seconds...\")\n",
    "                time.sleep(5 * attempt)  # Exponential backoff\n",
    "\n",
    "        if response is None:\n",
    "            print(f\"Failed to retrieve {category_link} after 3 attempts.\")\n",
    "            continue\n",
    "\n",
    "        # Parse the HTML content\n",
    "        data = bs(response.content, 'html.parser')\n",
    "        articles = data.find_all('tr', class_='topic-list-item')\n",
    "\n",
    "        for article in articles:\n",
    "            article_name_tag = article.find('a', class_='title')\n",
    "            article_name = article_name_tag.get_text(strip=True) if article_name_tag else \"N/A\"\n",
    "            article_link = article_name_tag['href'] if article_name_tag else \"N/A\"\n",
    "\n",
    "            # If the article link is relative, prepend the base URL\n",
    "            if article_link.startswith('/'):\n",
    "                article_link = f\"https://gov.optimism.io{article_link}\"\n",
    "\n",
    "            article_content = scrape_article_content(article_link, session)\n",
    "\n",
    "            # Save the data to the single CSV file\n",
    "            with open(output_file, 'a') as f:\n",
    "                f.write(f\"{category_name},{article_name},{article_link},{article_content}\\n\")\n",
    "\n",
    "            print(f\"Content for article '{article_name}' saved to {output_file}.\")\n",
    "\n",
    "scrape_all_article_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyCgsv4OA_ZlV-73_HN_Z1Jfhdtn5mibWq0\")\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Write a story about a magic backpack.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Why don't scientists trust atoms?\\n\\nBecause they make up everything! \\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 4,\n",
      "        \"candidates_token_count\": 15,\n",
      "        \"total_token_count\": 19\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the Gemini API\n",
    "genai.configure(api_key=\"AIzaSyCgsv4OA_ZlV-73_HN_Z1Jfhdtn5mibWq0\")  # Replace with your API key\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "df = pd.read_csv(\"all_articles_content.csv\")\n",
    "\n",
    "# Function to summarize article content\n",
    "def summarize_article(article_content):\n",
    "    prompt = f\"You are an expert in writing and have to summarize this content: {article_content}\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Create a new column for summaries\n",
    "df['Summary'] = df['Article'].apply(summarize_article)\n",
    "\n",
    "# Save the updated DataFrame back to Excel\n",
    "output_file_path = 'summarized_articles.xlsx'  # Specify the output file name\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(\"Summaries have been generated and saved to\", output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prodigal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
